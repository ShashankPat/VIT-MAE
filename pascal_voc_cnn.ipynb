{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU3tkS5GxvN1",
        "outputId": "151b2285-f01e-44a6-e935-318b0bcb5794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 16:33:47--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  20.8MB/s    in 95s     \n",
            "\n",
            "2024-12-13 16:35:22 (20.2 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "\n",
        "!tar -xf VOCtrainval_11-May-2012.tar -C /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class and colormap definitions for PASCAL VOC....\n",
        "VOC_CLASSES = [\n",
        "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
        "    \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
        "    \"motorbike\", \"person\", \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\"\n",
        "]\n",
        "\n",
        "VOC_COLORMAP = [\n",
        "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
        "    [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
        "    [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n",
        "    [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "    [0, 64, 128]\n",
        "]\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, root=\"/content/VOCdevkit/VOC2012\", is_train=True, transform=None, classes=None):\n",
        "        if is_train:\n",
        "            img_root = os.path.join(root, \"ImageSets\", \"Segmentation\", \"train.txt\")\n",
        "        else:\n",
        "            img_root = os.path.join(root, \"ImageSets\", \"Segmentation\", \"val.txt\")\n",
        "\n",
        "        img_names = []\n",
        "        with open(img_root, 'r') as rf:\n",
        "            names = [name.replace('\\n','') for name in rf.readlines()]\n",
        "            for name in names:\n",
        "                img_names.append(name)\n",
        "\n",
        "        self.classes = classes\n",
        "        self.transform = transform\n",
        "        self.img_names = img_names\n",
        "        self.root = root\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def _convert_to_segmentation_mask(self, mask):\n",
        "        height, width = mask.shape[:2]\n",
        "        segmentation_mask = np.zeros((height, width, len(VOC_COLORMAP)))\n",
        "\n",
        "        for label_index, label in enumerate(VOC_COLORMAP):\n",
        "            segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)\n",
        "\n",
        "        return segmentation_mask\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_name = self.img_names[item]\n",
        "        img = cv2.imread(os.path.join(self.root, \"JPEGImages\", img_name + \".jpg\"))\n",
        "        mask = cv2.imread(os.path.join(self.root, \"SegmentationClass\", img_name + \".png\"))\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert RGB mask to segmentation mask\n",
        "        mask = self._convert_to_segmentation_mask(mask)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "            mask = mask.argmax(dim=2).squeeze()\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjIyvBDy_lX",
        "outputId": "66aaa09d-c73c-440a-8f85-699663cb9d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "train_dataset = VOCDataset(root='/content/VOCdevkit/VOC2012', is_train=True, transform=transform, classes=VOC_CLASSES)\n",
        "val_dataset = VOCDataset(root='/content/VOCdevkit/VOC2012', is_train=False, transform=transform, classes=VOC_CLASSES)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJhEIsV7zFxL",
        "outputId": "b207e272-43c1-4f34-f2e0-b92e1ed6b959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1464\n",
            "Validation dataset size: 1449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification"
      ],
      "metadata": {
        "id": "Mrt59-Bvj76A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Classification Dataset..\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, root=\"./VOCdevkit/VOC2012\", classes=None, transform=None):\n",
        "        self.image_dir = os.path.join(root, \"JPEGImages\")\n",
        "        self.annotations_dir = os.path.join(root, \"Annotations\")\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = classes\n",
        "        self.transform = transform\n",
        "\n",
        "        for xml_file in os.listdir(self.annotations_dir):\n",
        "            file_path = os.path.join(self.annotations_dir, xml_file)\n",
        "            tree = ET.parse(file_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            filename = root.find(\"filename\").text\n",
        "            label_name = root.find(\"object\").find(\"name\").text\n",
        "\n",
        "            if label_name in self.classes:\n",
        "                self.image_paths.append(os.path.join(self.image_dir, filename))\n",
        "                self.labels.append(self.classes.index(label_name))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        return img, label\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
        "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "\n",
        "train_dataset = ClassificationDataset(root=\"./VOCdevkit/VOC2012\", classes=VOC_CLASSES, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "val_dataset = ClassificationDataset(root=\"./VOCdevkit/VOC2012\", classes=VOC_CLASSES, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# CNN-based Classification Model...\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 28 * 28, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = CNNClassifier(num_classes=len(VOC_CLASSES)).to(device)\n",
        "\n",
        "optimizer = Adam(cnn_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "def validate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "num_epochs = 8\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_model(cnn_model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_accuracy = validate_model(cnn_model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "torch.save(cnn_model.state_dict(), \"cnn_classification_model.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYhqRrbKzKEy",
        "outputId": "cb96b869-354e-42c6-93d4-4335339618eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Train Loss: 2.1581, Train Accuracy: 40.90%, Val Loss: 1.8314, Val Accuracy: 45.19%\n",
            "Epoch 2/8, Train Loss: 1.8049, Train Accuracy: 46.83%, Val Loss: 1.4099, Val Accuracy: 54.12%\n",
            "Epoch 3/8, Train Loss: 1.3831, Train Accuracy: 57.17%, Val Loss: 0.8492, Val Accuracy: 79.96%\n",
            "Epoch 4/8, Train Loss: 0.7894, Train Accuracy: 74.76%, Val Loss: 0.2439, Val Accuracy: 95.16%\n",
            "Epoch 5/8, Train Loss: 0.3287, Train Accuracy: 89.87%, Val Loss: 0.0726, Val Accuracy: 99.31%\n",
            "Epoch 6/8, Train Loss: 0.1600, Train Accuracy: 95.15%, Val Loss: 0.0281, Val Accuracy: 99.70%\n",
            "Epoch 7/8, Train Loss: 0.1066, Train Accuracy: 96.85%, Val Loss: 0.0178, Val Accuracy: 99.84%\n",
            "Epoch 8/8, Train Loss: 0.0827, Train Accuracy: 97.65%, Val Loss: 0.0092, Val Accuracy: 99.87%\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Segmentation"
      ],
      "metadata": {
        "id": "15-1_hW5j2oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "VOC_COLORMAP = [\n",
        "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
        "    [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
        "    [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n",
        "    [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "    [0, 64, 128]\n",
        "]\n",
        "\n",
        "# Map colormap to class indices...\n",
        "VOC_COLORMAP_TO_INDEX = {tuple(color): idx for idx, color in enumerate(VOC_COLORMAP)}\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root=\"./VOCdevkit/VOC2012\", transform=None):\n",
        "        self.image_dir = os.path.join(root, \"JPEGImages\")\n",
        "        self.mask_dir = os.path.join(root, \"SegmentationClass\")\n",
        "        self.image_paths = []\n",
        "        self.mask_paths = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for img_name in os.listdir(self.image_dir):\n",
        "            mask_path = os.path.join(self.mask_dir, img_name.replace(\".jpg\", \".png\"))\n",
        "            if os.path.exists(mask_path):\n",
        "                self.image_paths.append(os.path.join(self.image_dir, img_name))\n",
        "                self.mask_paths.append(mask_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _rgb_to_class_index(self, mask):\n",
        "        \"\"\"Convert RGB mask to class indices.\"\"\"\n",
        "        h, w, _ = mask.shape\n",
        "        class_indices = np.zeros((h, w), dtype=np.int64)\n",
        "        for rgb, idx in VOC_COLORMAP_TO_INDEX.items():\n",
        "            class_indices[(mask == rgb).all(axis=2)] = idx\n",
        "        return class_indices\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_paths[idx])\n",
        "\n",
        "        mask = self._rgb_to_class_index(mask)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return img, torch.tensor(mask, dtype=torch.long)\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "train_dataset = SegmentationDataset(root=\"./VOCdevkit/VOC2012\", transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "val_dataset = SegmentationDataset(root=\"./VOCdevkit/VOC2012\", transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# U-Net-like Encoder-Decoder Architecture...\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        self.encoder1 = conv_block(3, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.bottleneck = conv_block(256, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        bottleneck = self.bottleneck(self.pool3(enc3))\n",
        "\n",
        "        dec3 = self.decoder3(torch.cat((self.upconv3(bottleneck), enc3), dim=1))\n",
        "        dec2 = self.decoder2(torch.cat((self.upconv2(dec3), enc2), dim=1))\n",
        "        dec1 = self.decoder1(torch.cat((self.upconv1(dec2), enc1), dim=1))\n",
        "\n",
        "        return F.interpolate(self.final_conv(dec1), size=(224, 224), mode=\"bilinear\")\n",
        "\n",
        "num_classes = 21\n",
        "model = UNet(num_classes).to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "def train_segmentation(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, masks in loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        total_correct += (predictions == masks).sum().item()\n",
        "        total_samples += masks.numel()\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "def validate_segmentation(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            total_correct += (predictions == masks).sum().item()\n",
        "            total_samples += masks.numel()\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    return total_loss / len(loader), accuracy\n",
        "\n",
        "num_epochs = 8\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_segmentation(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_accuracy = validate_segmentation(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"cnn_segmentation_model.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "DvSz0TFoA4TT",
        "outputId": "17a18c05-edaa-4948-d13e-23fe282bffdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6830bbd46092>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return img, torch.tensor(mask, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Train Loss: 0.8117, Train Acc: 86.86%, Val Loss: 0.5661, Val Acc: 90.39%\n",
            "Epoch 2/8, Train Loss: 0.5379, Train Acc: 90.39%, Val Loss: 0.5289, Val Acc: 90.39%\n",
            "Epoch 3/8, Train Loss: 0.5250, Train Acc: 90.39%, Val Loss: 0.5173, Val Acc: 90.39%\n",
            "Epoch 4/8, Train Loss: 0.5186, Train Acc: 90.39%, Val Loss: 0.5102, Val Acc: 90.39%\n",
            "Epoch 5/8, Train Loss: 0.5141, Train Acc: 90.39%, Val Loss: 0.5054, Val Acc: 90.39%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6830bbd46092>\u001b[0m in \u001b[0;36m<cell line: 188>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6830bbd46092>\u001b[0m in \u001b[0;36mtrain_segmentation\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}